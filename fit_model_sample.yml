
# set to True for a debug run
# debug: True

# set a directory and file name for the log-file for this run

log_dir: '../test_runs/'
log_file: 'test_run' # don't use a file extension; '.log' will be added automatically
# the actual file name will be appended with a time string in the directory log_dir

checkpoint_dir: '../test_checkpoints/' # directory where checkpoints will be written
# checkpoints will be named checkpoint_'timestring'.js where timestring is date & time
# when the job was run

# a log message for the run
log_message: 'Hello, HyperGraphs of the World, unite!'

# path to the data base containing the pickled hypergraphs used for training/validation/testing

database: "../Databases/QM9ERHGraphDataBase/"

n_training_max: 5000
n_test_max: 1000
train_validation_fraction: 0.3
training_batch_size: 50
validation_batch_size: 50

# In the following we define the hypergraph convolution model, which has three parts, namely:
# 1) we start by giving a PRNGKey (an integer) for parameter initialisation

key: 42

# 2) graph convolution layers: specified by a list of dicts, with each dict giving
#    the input and output dimensions of the hedge and node features; if output dimensions are
#    not specified they are taken to be equal to the corresponding input

# construct a model with three hypergraph convolution layers
# below each line starting with '-' indicates a new layer

# n_hedge_in and n_node_in in the first convolution must be read from the information in 
# the database, so the numbers given here as input sizes for the first layer will actually
# be ignored if they are different from what the database was constructed with
convolution_layers:
  - n_hedge_in: 1
    n_node_in: 1
    n_hedge_out: 60    # if undefined, taken equal to n_hedge_in
    n_node_out: 60     # if undefined, taken equal to n_node_in
  - n_hedge_in: 60
    n_node_in: 60
  - n_hedge_in: 60
    n_node_in: 60
    n_hedge_out: 60
    n_node_out: 60

# 3) Now define the subsequent multi-layer perceptrons for hedges and nodes;
# note that the input layer has to match in size the output sizes of the 
# last convolution layer for hedges and nodes, respectively
# Also note that the last output size is 1 (for predicting energy)
# As for the convolution part, each '-' signals the start of a new MLP layer

hedge_MLP:
  - n_hedge_in: 60
    n_hedge_out: 60
  - n_hedge_in: 60
    n_hedge_out: 60
  - n_hedge_in: 60
    n_hedge_out: 1
      
node_MLP:
  - n_node_in: 60
    n_node_out: 60
  - n_node_in: 60
    n_node_out: 60
  - n_node_in: 60
    n_node_out: 1
      
# define the training step

n_epochs: 100
n_print: 10 # frequency of training/validation loss output
n_checkpoint_freq: 10
learning_rate: 1.5e-2
lr_reduction_factor: 0.1
lr_patience: 10
lr_rtol: 1.0e-4
lr_atol: 1.0e-5
