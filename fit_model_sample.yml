
# set to True for a debug run
debug: True

# path to the data base containing the pickled hypergraphs used for training/validation/testing

database: "../Databases/QM9ERHGraphDataBase/"

n_training_max: 5000
n_test_max: 1500
train_validation_fraction: 0.3

# In the following we define the hypergraph convolution model, which has three parts, namely:
# 1) we start by giving a PRNGKey (an integer) for parameter initialisation

key: 42

# 2) graph convolution layers: specified by a list of dicts, with each dict giving
#    the input and output dimensions of the hedge and node features; if output dimensions are
#    not specified they are taken to be equal to the corresponding input

# construct a model with three hypergraph convolution layers
# below each line starting with '-' indicates a new layer

convolution_layers:
  - n_hedge_in: 30
    n_node_in: 30
    n_hedge_out: 60    # if undefined, taken equal to n_hedge_in
    n_node_out: 60     # if undefined, taken equal to n_node_in
  - n_hedge_in: 60
    n_node_in: 60
  - n_hedge_in: 60
    n_node_in: 60
    n_hedge_out: 60
    n_node_out: 60

# 3) Now define the subsequent multi-layer perceptrons for hedges and nodes;
# note that the input layer has to match in size the output sizes of the 
# last convolution layer for hedges and nodes, respectively
# Also note that the last output size is 1 (for predicting energy)
# As for the convolution part, each '-' signals the start of a new MLP layer

hedge_MLP:
  - n_hedge_in: 60
    n_hedge_out: 60
  - n_hedge_in: 60
    n_hedge_out: 60
  - n_hedge_in: 60
    n_hedge_out: 1
      
node_MLP:
  - n_node_in: 60
    n_node_out: 60
  - n_node_in: 60
    n_node_out: 60
  - n_node_in: 60
    n_node_out: 1
      
# define the training step

n_epochs: 100
n_batch: 50
n_checkpoint_freq: 10
learning_rate: 1.0e-3
n_training_max: 5000
n_test_max: 500
